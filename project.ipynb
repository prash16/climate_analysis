{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate Model Simulation Crashes Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Libraries \n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.linalg as npla\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from collections import deque\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet ,LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss, mean_squared_error, zero_one_loss\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score,GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "# loading R \n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Reading the data set \n",
    "library(data.table)\n",
    "dataset<-read.table(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat\", \n",
    "           header=TRUE)\n",
    "# writing the data set \n",
    "write.csv(dataset, file = \"climate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reading the dataset \n",
    "\n",
    "climate_data=pd.read_csv('climate.csv')\n",
    "X = climate_data.iloc[:,3:20].values\n",
    "y = climate_data['outcome'].values\n",
    "\n",
    "# Splitting the dataset in 25% train and test \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(540, 17)\n",
      "(540,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Study</th>\n",
       "      <th>Run</th>\n",
       "      <th>vconst_corr</th>\n",
       "      <th>vconst_2</th>\n",
       "      <th>vconst_3</th>\n",
       "      <th>vconst_4</th>\n",
       "      <th>vconst_5</th>\n",
       "      <th>vconst_7</th>\n",
       "      <th>ah_corr</th>\n",
       "      <th>...</th>\n",
       "      <th>efficiency_factor</th>\n",
       "      <th>tidal_mix_max</th>\n",
       "      <th>vertical_decay_scale</th>\n",
       "      <th>convect_corr</th>\n",
       "      <th>bckgrnd_vdc1</th>\n",
       "      <th>bckgrnd_vdc_ban</th>\n",
       "      <th>bckgrnd_vdc_eq</th>\n",
       "      <th>bckgrnd_vdc_psim</th>\n",
       "      <th>Prandtl</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.859036</td>\n",
       "      <td>0.927825</td>\n",
       "      <td>0.252866</td>\n",
       "      <td>0.298838</td>\n",
       "      <td>0.170521</td>\n",
       "      <td>0.735936</td>\n",
       "      <td>0.428325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245675</td>\n",
       "      <td>0.104226</td>\n",
       "      <td>0.869091</td>\n",
       "      <td>0.997518</td>\n",
       "      <td>0.448620</td>\n",
       "      <td>0.307522</td>\n",
       "      <td>0.858310</td>\n",
       "      <td>0.796997</td>\n",
       "      <td>0.869893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.606041</td>\n",
       "      <td>0.457728</td>\n",
       "      <td>0.359448</td>\n",
       "      <td>0.306957</td>\n",
       "      <td>0.843331</td>\n",
       "      <td>0.934851</td>\n",
       "      <td>0.444572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.616870</td>\n",
       "      <td>0.975786</td>\n",
       "      <td>0.914344</td>\n",
       "      <td>0.845247</td>\n",
       "      <td>0.864152</td>\n",
       "      <td>0.346713</td>\n",
       "      <td>0.356573</td>\n",
       "      <td>0.438447</td>\n",
       "      <td>0.512256</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.997600</td>\n",
       "      <td>0.373238</td>\n",
       "      <td>0.517399</td>\n",
       "      <td>0.504993</td>\n",
       "      <td>0.618903</td>\n",
       "      <td>0.605571</td>\n",
       "      <td>0.746225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.679355</td>\n",
       "      <td>0.803413</td>\n",
       "      <td>0.643995</td>\n",
       "      <td>0.718441</td>\n",
       "      <td>0.924775</td>\n",
       "      <td>0.315371</td>\n",
       "      <td>0.250642</td>\n",
       "      <td>0.285636</td>\n",
       "      <td>0.365858</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.783408</td>\n",
       "      <td>0.104055</td>\n",
       "      <td>0.197533</td>\n",
       "      <td>0.421837</td>\n",
       "      <td>0.742056</td>\n",
       "      <td>0.490828</td>\n",
       "      <td>0.005525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.471463</td>\n",
       "      <td>0.597879</td>\n",
       "      <td>0.761659</td>\n",
       "      <td>0.362751</td>\n",
       "      <td>0.912819</td>\n",
       "      <td>0.977971</td>\n",
       "      <td>0.845921</td>\n",
       "      <td>0.699431</td>\n",
       "      <td>0.475987</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.513199</td>\n",
       "      <td>0.061812</td>\n",
       "      <td>0.635837</td>\n",
       "      <td>0.844798</td>\n",
       "      <td>0.441502</td>\n",
       "      <td>0.191926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551543</td>\n",
       "      <td>0.743877</td>\n",
       "      <td>0.312349</td>\n",
       "      <td>0.650223</td>\n",
       "      <td>0.522261</td>\n",
       "      <td>0.043545</td>\n",
       "      <td>0.376660</td>\n",
       "      <td>0.280098</td>\n",
       "      <td>0.132283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Study  Run  vconst_corr  vconst_2  vconst_3  vconst_4  \\\n",
       "0           1      1    1     0.859036  0.927825  0.252866  0.298838   \n",
       "1           2      1    2     0.606041  0.457728  0.359448  0.306957   \n",
       "2           3      1    3     0.997600  0.373238  0.517399  0.504993   \n",
       "3           4      1    4     0.783408  0.104055  0.197533  0.421837   \n",
       "4           5      1    5     0.406250  0.513199  0.061812  0.635837   \n",
       "\n",
       "   vconst_5  vconst_7   ah_corr   ...     efficiency_factor  tidal_mix_max  \\\n",
       "0  0.170521  0.735936  0.428325   ...              0.245675       0.104226   \n",
       "1  0.843331  0.934851  0.444572   ...              0.616870       0.975786   \n",
       "2  0.618903  0.605571  0.746225   ...              0.679355       0.803413   \n",
       "3  0.742056  0.490828  0.005525   ...              0.471463       0.597879   \n",
       "4  0.844798  0.441502  0.191926   ...              0.551543       0.743877   \n",
       "\n",
       "   vertical_decay_scale  convect_corr  bckgrnd_vdc1  bckgrnd_vdc_ban  \\\n",
       "0              0.869091      0.997518      0.448620         0.307522   \n",
       "1              0.914344      0.845247      0.864152         0.346713   \n",
       "2              0.643995      0.718441      0.924775         0.315371   \n",
       "3              0.761659      0.362751      0.912819         0.977971   \n",
       "4              0.312349      0.650223      0.522261         0.043545   \n",
       "\n",
       "   bckgrnd_vdc_eq  bckgrnd_vdc_psim   Prandtl  outcome  \n",
       "0        0.858310          0.796997  0.869893        0  \n",
       "1        0.356573          0.438447  0.512256        1  \n",
       "2        0.250642          0.285636  0.365858        1  \n",
       "3        0.845921          0.699431  0.475987        1  \n",
       "4        0.376660          0.280098  0.132283        1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some initial data analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate_data['outcome'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1                                        Visualize the data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vconst_corr,vconst_2,vconst_3,vconst_4,vconst_5,vconst_7,\n",
    "#ah_corr,ah_bolus,slm_corr,efficiency_factor,tidal_mix_max,\n",
    "#vertical_decay_scale,convect_corr,bckgrnd_vdc1,bckgrnd_vdc_ban,bckgrnd_vdc_eq,bckgrnd_vdc_psim,Prandtl,outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 Model 1  DT classifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for decision tree for climate data  = 0.874074\n"
     ]
    }
   ],
   "source": [
    "## Decision Trees for cancer\n",
    "\n",
    "#We'll start just by training a single decision tree.\n",
    "\n",
    "error_rate_testing_sciktlearn_cancer_decisiontree=[]\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train,y_train)\n",
    "\n",
    "## Prediction and Evaluation \n",
    "\n",
    "#Let's evaluate our decision tree.\n",
    "predictions_dt_climate = dtree.predict(X_test)\n",
    "\n",
    "Test_error_climate_dt=zero_one_loss(y_test,predictions_dt_climate)\n",
    "Accuracy_test_climate_dt=1-Test_error_cancer_dt\n",
    "\n",
    "#print('Test error for decision tree for climate data  = %f' % Test_error_climate_dt)\n",
    "print('Test Accuracy for decision tree for climate data  = %f' % Accuracy_test_climate_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135,)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_dt_climate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 Model 2 random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.918519\n"
     ]
    }
   ],
   "source": [
    "# random forest model \n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "smse = sum(rfc_pred != y_test) / len(y_test)\n",
    "\n",
    "Accuracy = 1-smse\n",
    "\n",
    "#print('Test error = %f' % smse)\n",
    "print('Accuracy = %f' % Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91851851851851851"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 Model 3 Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "      <th>mean_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>200.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.948181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.948181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>200.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.948164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.946329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.946329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.946329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.946329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.946329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.946329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.946329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.946312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.946312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>200.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.946295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.944494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.944494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.944494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.944477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.944477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.944477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>200.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.944477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>200.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.944477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.944477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>200.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.944460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.944460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.944460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>200.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.944460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.942677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.942660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.942643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.942626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.929610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.927775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.925923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>200.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>200.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.914829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.912977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  max_depth  max_features  mean_score\n",
       "44         200.0       10.0           8.0    0.948181\n",
       "35         200.0        5.0          15.0    0.948181\n",
       "46         200.0       10.0          12.0    0.948164\n",
       "70        1000.0       10.0          12.0    0.946329\n",
       "59        1000.0        5.0          15.0    0.946329\n",
       "33         200.0        5.0          10.0    0.946329\n",
       "9          100.0        5.0          10.0    0.946329\n",
       "20         100.0       10.0           8.0    0.946329\n",
       "17         100.0        8.0          15.0    0.946329\n",
       "14         100.0        8.0           8.0    0.946329\n",
       "15         100.0        8.0          10.0    0.946312\n",
       "34         200.0        5.0          12.0    0.946312\n",
       "39         200.0        8.0          10.0    0.946295\n",
       "23         100.0       10.0          15.0    0.944494\n",
       "71        1000.0       10.0          15.0    0.944494\n",
       "65        1000.0        8.0          15.0    0.944494\n",
       "64        1000.0        8.0          12.0    0.944477\n",
       "22         100.0       10.0          12.0    0.944477\n",
       "10         100.0        5.0          12.0    0.944477\n",
       "40         200.0        8.0          12.0    0.944477\n",
       "47         200.0       10.0          15.0    0.944477\n",
       "58        1000.0        5.0          12.0    0.944477\n",
       "45         200.0       10.0          10.0    0.944460\n",
       "21         100.0       10.0          10.0    0.944460\n",
       "56        1000.0        5.0           8.0    0.944460\n",
       "38         200.0        8.0           8.0    0.944460\n",
       "11         100.0        5.0          15.0    0.942677\n",
       "32         200.0        5.0           8.0    0.942660\n",
       "69        1000.0       10.0          10.0    0.942643\n",
       "57        1000.0        5.0          10.0    0.942626\n",
       "..           ...        ...           ...         ...\n",
       "31         200.0        5.0           5.0    0.929610\n",
       "7          100.0        5.0           5.0    0.927775\n",
       "55        1000.0        5.0           5.0    0.925923\n",
       "66        1000.0       10.0           1.0    0.914829\n",
       "60        1000.0        8.0           1.0    0.914829\n",
       "51        1000.0        2.0          10.0    0.914829\n",
       "54        1000.0        5.0           1.0    0.914829\n",
       "53        1000.0        2.0          15.0    0.914829\n",
       "52        1000.0        2.0          12.0    0.914829\n",
       "0          100.0        2.0           1.0    0.914829\n",
       "50        1000.0        2.0           8.0    0.914829\n",
       "49        1000.0        2.0           5.0    0.914829\n",
       "2          100.0        2.0           8.0    0.914829\n",
       "3          100.0        2.0          10.0    0.914829\n",
       "4          100.0        2.0          12.0    0.914829\n",
       "6          100.0        5.0           1.0    0.914829\n",
       "12         100.0        8.0           1.0    0.914829\n",
       "18         100.0       10.0           1.0    0.914829\n",
       "24         200.0        2.0           1.0    0.914829\n",
       "25         200.0        2.0           5.0    0.914829\n",
       "26         200.0        2.0           8.0    0.914829\n",
       "27         200.0        2.0          10.0    0.914829\n",
       "28         200.0        2.0          12.0    0.914829\n",
       "29         200.0        2.0          15.0    0.914829\n",
       "30         200.0        5.0           1.0    0.914829\n",
       "1          100.0        2.0           5.0    0.914829\n",
       "42         200.0       10.0           1.0    0.914829\n",
       "48        1000.0        2.0           1.0    0.914829\n",
       "36         200.0        8.0           1.0    0.914829\n",
       "5          100.0        2.0          15.0    0.912977\n",
       "\n",
       "[72 rows x 4 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators = [100,200,1000]\n",
    "max_depth = [2,5,8,10]\n",
    "max_features = [1,5,8,10,12,15]\n",
    "results = pd.DataFrame(columns=['n_estimators','max_depth','max_features','mean_score'])\n",
    "\n",
    "for n in n_estimators:\n",
    "    for m in max_depth:\n",
    "        for f in max_features:\n",
    "            clf = RFC(n_estimators=n,max_depth=m,max_features=f,criterion='entropy')\n",
    "            scores = cross_val_score(clf,X,y,cv=5)\n",
    "            results = results.append({'n_estimators':n,'max_depth':m,'max_features':f,'mean_score':scores.mean()},ignore_index=True)\n",
    "    \n",
    "results.sort_values(by='mean_score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5 Model 4 Backward selection using  L1 and L2 penealty  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " L1 regularization: no of features  12\n",
      " L1 regularization: error  0.955555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "#penalty l1\n",
    "model_l1 = LogisticRegression(penalty = 'l1')\n",
    "\n",
    "# Fitting the model \n",
    "model_l1.fit(X_train,y_train)\n",
    "\n",
    "pred = model_l1.predict(X_test)\n",
    "\n",
    "# coefficient calculation and error calculation \n",
    "\n",
    "sf_model = np.sum(model_l1.coef_/max(model_l1.coef_) >= 1e-6)\n",
    "v_error = np.mean(model_l1.predict(X_test) != y_test)\n",
    "\n",
    "accuracy = np.mean(pred==y_test)\n",
    "# printing the features, and error\n",
    "\n",
    "print(' L1 regularization: no of features ', sf_model)\n",
    "print(' L1 regularization: error ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.48289529, -4.38849205,  0.        ,  1.13591588,  1.55448867,\n",
       "         0.50366527,  0.        ,  0.04760432,  0.25163128, -0.1593905 ,\n",
       "         0.        ,  0.        , -2.88605803,  2.36634888,  0.        ,\n",
       "         1.32179841,  1.00965738]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_l1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " L2 regularization: no of features  16\n",
      " L2 regularization: error  0.940740740741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#penalty l1\n",
    "\n",
    "model_l2 = LogisticRegression(penalty = 'l2')\n",
    "\n",
    "# Fitting the model \n",
    "model_l2.fit(X_train,y_train)\n",
    "\n",
    "pred = model_l2.predict(X_test)\n",
    "\n",
    "# coefficient calculation and error calculation \n",
    "\n",
    "sf_model = np.sum(model_l2.coef_/max(model_l1.coef_) >= 1e-6)\n",
    "v_error = np.mean(model_l2.predict(X_test) != y_test)\n",
    "\n",
    "accuracy = np.mean(pred==y_test)\n",
    "# printing the features, and error\n",
    "\n",
    "print(' L2 regularization: no of features ', sf_model)\n",
    "print(' L2 regularization: error ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.64883439e+00,  -2.52086837e+00,   2.01468085e-03,\n",
       "          1.19117186e+00,   1.35121467e+00,   8.00429543e-01,\n",
       "          3.23026739e-01,   4.92359467e-01,   7.54589028e-01,\n",
       "         -3.03693666e-01,   2.35828292e-01,  -5.41674959e-02,\n",
       "         -1.96728467e+00,   1.89172736e+00,   5.95516642e-03,\n",
       "          1.24863310e+00,   1.05562686e+00]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_l2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an empty list to hold train error, and one to hold test error\n",
    "\n",
    "trainer = list()\n",
    "valerr = list()\n",
    "penalty=['l1','l2']\n",
    "C = np.arange(1,3,0.1)\n",
    "\n",
    "# make an array of the number of features to sweep through\n",
    "\n",
    "feats = np.arange(1,17)\n",
    "\n",
    "# loop over the features, and fit the model each time\n",
    "# also calculate train and test error\n",
    "\n",
    "for p in penalty:\n",
    "    for c in C:\n",
    "        for f in feats:\n",
    "            rfe = RFE(estimator = Ridge(), n_features_to_select = f)\n",
    "            rfe.fit(X_train, y_train)\n",
    "            Xtrain_sel = X_train[:,rfe.support_]\n",
    "            Xvalidate_sel = X_test[:,rfe.support_]\n",
    "            model_l1 = LogisticRegression(penalty = p,C=c)\n",
    "            model_l1.fit(Xtrain_sel,y_train)\n",
    "            \n",
    "            v_error_l1 = np.mean(model_l1.predict(Xvalidate_sel) != y_test)\n",
    "            train_error_l1 = np.mean(model_l1.predict(Xtrain_sel) != y_train)\n",
    "            \n",
    "            trainer.append(train_error_l1)\n",
    "            valerr.append(v_error_l1)\n",
    "            \n",
    "            results = results.append({'penalty':p,'C':c,'features':f,'training error ':trainer.append(train_error_l1)},\n",
    "                                     ignore_index=True)\n",
    "\n",
    "# coefficient calculation and error calculation \n",
    "\n",
    "    #sf_model_l2 = np.sum(model_l2.coef_/max(model_l2.coef_) >= 1e-6)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>C</th>\n",
       "      <th>features</th>\n",
       "      <th>penalty</th>\n",
       "      <th>training error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.885147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.866629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.931497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.933399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.931513</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.933417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.929662</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.931514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.924089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.924089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.920402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.918550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.916681</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.918533</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.914829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.914829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.914829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.914829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.914829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>l1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>l1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>l1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>l1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>l1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>l1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>l1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>l1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>l1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>l1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>l1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>9.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>11.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>12.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>15.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>13.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>14.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>659 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     n_neighbors  mean_score    C  features penalty training error \n",
       "0            1.0    0.885147  NaN       NaN     NaN             NaN\n",
       "1            2.0    0.866629  NaN       NaN     NaN             NaN\n",
       "2            3.0    0.931497  NaN       NaN     NaN             NaN\n",
       "3            4.0    0.933399  NaN       NaN     NaN             NaN\n",
       "4            5.0    0.931513  NaN       NaN     NaN             NaN\n",
       "5            6.0    0.933417  NaN       NaN     NaN             NaN\n",
       "6            7.0    0.929662  NaN       NaN     NaN             NaN\n",
       "7            8.0    0.931514  NaN       NaN     NaN             NaN\n",
       "8            9.0    0.924089  NaN       NaN     NaN             NaN\n",
       "9           10.0    0.924089  NaN       NaN     NaN             NaN\n",
       "10          11.0    0.920402  NaN       NaN     NaN             NaN\n",
       "11          12.0    0.918550  NaN       NaN     NaN             NaN\n",
       "12          13.0    0.916681  NaN       NaN     NaN             NaN\n",
       "13          14.0    0.918533  NaN       NaN     NaN             NaN\n",
       "14          15.0    0.914829  NaN       NaN     NaN             NaN\n",
       "15          16.0    0.914829  NaN       NaN     NaN             NaN\n",
       "16          17.0    0.914829  NaN       NaN     NaN             NaN\n",
       "17          18.0    0.914829  NaN       NaN     NaN             NaN\n",
       "18          19.0    0.914829  NaN       NaN     NaN             NaN\n",
       "19           NaN         NaN  1.0       1.0      l1            None\n",
       "20           NaN         NaN  1.0       2.0      l1            None\n",
       "21           NaN         NaN  1.0       3.0      l1            None\n",
       "22           NaN         NaN  1.0       4.0      l1            None\n",
       "23           NaN         NaN  1.0       5.0      l1            None\n",
       "24           NaN         NaN  1.0       6.0      l1            None\n",
       "25           NaN         NaN  1.0       7.0      l1            None\n",
       "26           NaN         NaN  1.0       8.0      l1            None\n",
       "27           NaN         NaN  1.0       9.0      l1            None\n",
       "28           NaN         NaN  1.0      10.0      l1            None\n",
       "29           NaN         NaN  1.0      11.0      l1            None\n",
       "..           ...         ...  ...       ...     ...             ...\n",
       "629          NaN         NaN  2.8       3.0      l2            None\n",
       "630          NaN         NaN  2.8       4.0      l2            None\n",
       "631          NaN         NaN  2.8       5.0      l2            None\n",
       "632          NaN         NaN  2.8       6.0      l2            None\n",
       "633          NaN         NaN  2.8       7.0      l2            None\n",
       "634          NaN         NaN  2.8       8.0      l2            None\n",
       "635          NaN         NaN  2.8       9.0      l2            None\n",
       "636          NaN         NaN  2.8      10.0      l2            None\n",
       "637          NaN         NaN  2.8      11.0      l2            None\n",
       "638          NaN         NaN  2.8      12.0      l2            None\n",
       "639          NaN         NaN  2.8      13.0      l2            None\n",
       "640          NaN         NaN  2.8      14.0      l2            None\n",
       "641          NaN         NaN  2.8      15.0      l2            None\n",
       "642          NaN         NaN  2.8      16.0      l2            None\n",
       "643          NaN         NaN  2.9       1.0      l2            None\n",
       "644          NaN         NaN  2.9       2.0      l2            None\n",
       "645          NaN         NaN  2.9       3.0      l2            None\n",
       "646          NaN         NaN  2.9       4.0      l2            None\n",
       "647          NaN         NaN  2.9       5.0      l2            None\n",
       "648          NaN         NaN  2.9       6.0      l2            None\n",
       "649          NaN         NaN  2.9       7.0      l2            None\n",
       "650          NaN         NaN  2.9       8.0      l2            None\n",
       "651          NaN         NaN  2.9       9.0      l2            None\n",
       "652          NaN         NaN  2.9      10.0      l2            None\n",
       "653          NaN         NaN  2.9      11.0      l2            None\n",
       "654          NaN         NaN  2.9      12.0      l2            None\n",
       "655          NaN         NaN  2.9      13.0      l2            None\n",
       "656          NaN         NaN  2.9      14.0      l2            None\n",
       "657          NaN         NaN  2.9      15.0      l2            None\n",
       "658          NaN         NaN  2.9      16.0      l2            None\n",
       "\n",
       "[659 rows x 6 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Ref : https://github.com/ubcs3/2017-Fall/blob/master/notes-2017-10-13/notes-2017-10-13-ideas.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11edb85f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# make an empty list to hold train error, and one to hold test error\n",
    "trainer = list()\n",
    "valerr = list()\n",
    "\n",
    "# make an array of the number of features to sweep through\n",
    "feats = np.arange(1,17)\n",
    "\n",
    "# loop over the features, and fit the model each time\n",
    "# also calculate train and test error\n",
    "for f in feats:\n",
    "    rfe = RFE(estimator = Ridge(), n_features_to_select = f)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    Xtrain_sel = X_train[:,rfe.support_]\n",
    "    Xvalidate_sel = X_test[:,rfe.support_]\n",
    "    \n",
    "    model_l2 = LogisticRegression(penalty = 'l1')\n",
    "\n",
    "    # Fitting the model \n",
    "    model_l2.fit(Xtrain_sel,y_train)\n",
    "\n",
    "# coefficient calculation and error calculation \n",
    "\n",
    "    #sf_model_l2 = np.sum(model_l2.coef_/max(model_l2.coef_) >= 1e-6)\n",
    "    v_error_l2 = np.mean(model_l2.predict(Xvalidate_sel) != y_test)\n",
    "    train_error_l2 = np.mean(model_l2.predict(Xtrain_sel) != y_train)\n",
    "    \n",
    "    \n",
    "    trainer.append(train_error_l2)\n",
    "    valerr.append(v_error_l2)\n",
    "      \n",
    "    \n",
    "       # plot validation loss\n",
    "plt.plot(np.arange(1,17), trainer, label=\"train\")\n",
    "plt.plot(np.arange(1,17), valerr, label=\"test\")\n",
    "plt.xlabel(\"feats\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 Model 5 CV using L1 and L2 penealty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>penalty</th>\n",
       "      <th>C</th>\n",
       "      <th>fit_intercept</th>\n",
       "      <th>mean_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>l1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.961144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>l1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>True</td>\n",
       "      <td>0.961144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>l1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.961144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>l1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.961144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>l1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.961144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>l1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.961144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>l1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.961144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>l1</td>\n",
       "      <td>1.7</td>\n",
       "      <td>True</td>\n",
       "      <td>0.959293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>l1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>True</td>\n",
       "      <td>0.959293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>l1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>True</td>\n",
       "      <td>0.959293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   penalty    C fit_intercept  mean_score\n",
       "70      l1  2.5          True    0.961144\n",
       "72      l1  2.6          True    0.961144\n",
       "66      l1  2.3          True    0.961144\n",
       "60      l1  2.0          True    0.961144\n",
       "68      l1  2.4          True    0.961144\n",
       "64      l1  2.2          True    0.961144\n",
       "62      l1  2.1          True    0.961144\n",
       "54      l1  1.7          True    0.959293\n",
       "56      l1  1.8          True    0.959293\n",
       "74      l1  2.7          True    0.959293"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty = ['l2','l1']\n",
    "C = np.arange(1,3,0.1)\n",
    "fit_intercept = [True,False]\n",
    "results = pd.DataFrame(columns=['penalty','C','fit_intercept','mean_score'])\n",
    "\n",
    "for p in penalty:\n",
    "    for c in C:\n",
    "        for f in fit_intercept:\n",
    "            reg = LogisticRegression(penalty=p,C=c,fit_intercept=f)\n",
    "            scores = cross_val_score(reg,X,y,cv=5)\n",
    "            results = results.append({'penalty':p,'C':c,'fit_intercept':f,'mean_score':scores.mean()},ignore_index=True)\n",
    "    \n",
    "results.sort_values(by='mean_score',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>mean_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.933417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.933399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.931514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.931513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.931497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.929662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.924089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.924089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.920402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.918550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_neighbors  mean_score\n",
       "5           6.0    0.933417\n",
       "3           4.0    0.933399\n",
       "7           8.0    0.931514\n",
       "4           5.0    0.931513\n",
       "2           3.0    0.931497\n",
       "6           7.0    0.929662\n",
       "9          10.0    0.924089\n",
       "8           9.0    0.924089\n",
       "10         11.0    0.920402\n",
       "11         12.0    0.918550"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neighbors = range(1,20)\n",
    "results = pd.DataFrame(columns=['n_neighbors','mean_score'])\n",
    "\n",
    "for n in n_neighbors:\n",
    "    clf = KNC(n_neighbors=n)\n",
    "    scores = cross_val_score(clf,X,y,cv=5)\n",
    "    results = results.append({'n_neighbors':n,'mean_score':scores.mean()},ignore_index=True)\n",
    "    \n",
    "results.sort_values(by='mean_score',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# .cient of model which are not zero are selected ....print out the coefficient and check values \n",
    "\n",
    "# which features are good to predict \n",
    "\n",
    "# binary ...how balance is your data ...and talk about that \n",
    "\n",
    "# visualizatiob on bar graph "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
